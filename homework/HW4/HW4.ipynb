{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Input\n",
    "user_item_rating = {}\n",
    "item_user_rating = {}\n",
    "item_content = {}\n",
    "total_rating = []\n",
    "vocab = []\n",
    "test = []\n",
    "with open('input.txt') as f:\n",
    "    R,M = f.readline().strip().split(' ')\n",
    "    for i,lines in enumerate(f):\n",
    "        line = lines.strip().split(' ')\n",
    "        if  i < int(R):\n",
    "            user,item,rating = int(line[0]),int(line[1]),float(line[2])\n",
    "            total_rating.append(float(rating))\n",
    "            if user not in user_item_rating:\n",
    "                user_item_rating[user] = {item:rating}\n",
    "            else:\n",
    "                user_item_rating[user][item] = rating\n",
    "            if item not in item_user_rating:\n",
    "                item_user_rating[item] = {user:rating}\n",
    "            else:\n",
    "                item_user_rating[item][user] = rating\n",
    "\n",
    "        if  i >= int(R) and i < int(R) + int(M):\n",
    "            item,content = int(line[0]),line[1:]\n",
    "            item_content[item] = content\n",
    "            \n",
    "        if  i >= int(R) + int(M):\n",
    "            test.append((line[0],line[1]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean\n",
    "mu = sum(total_rating)/len(total_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bias\n",
    "b_i = {item:0 for item in item_user_rating}\n",
    "b_u = {user:0 for user in user_item_rating}\n",
    "\n",
    "for i in item_user_rating.keys():\n",
    "    rating = []\n",
    "    for u in item_user_rating[i]:\n",
    "        rating.append(item_user_rating[i][u] - mu)\n",
    "    b_i[i] = sum(rating)/len(rating)\n",
    "    \n",
    "for u in user_item_rating.keys():\n",
    "    rating = []\n",
    "    for i in user_item_rating[u]:\n",
    "        rating.append(user_item_rating[u][i] - mu - b_i[i])\n",
    "    b_u[u] = sum(rating)/len(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarity_function\n",
    "def Pearson_correlation(i,j):\n",
    "    U_ij = set(item_user_rating[i].keys())&set(item_user_rating[j].keys())\n",
    "    if len(U_ij) == 0:\n",
    "        return 0\n",
    "    top = 0\n",
    "    bot = 0\n",
    "    for u in U_ij:\n",
    "        b_ui = b_u[u]+b_i[i]+mu\n",
    "        b_uj = b_u[u]+b_i[j]+mu\n",
    "        top += (user_item_rating[u][i]-b_ui)*(user_item_rating[u][j]-b_uj)\n",
    "        bot += (user_item_rating[u][i]-b_ui)**2*(user_item_rating[u][j]-b_uj)**2\n",
    "    s_ij = top/(bot)**(1/2)\n",
    "    return(s_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_content = ' '.join([' '.join(list(set(content))) for content in item_content.values()])\n",
    "df = Counter(total_content.split())\n",
    "vocab = list(df.keys())\n",
    "N = len(item_content)\n",
    "idf = {x: np.log(N/(df[x])+1) for x in df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(i):\n",
    "    tf_top_i = Counter(item_content[i])\n",
    "    d_i = np.zeros(len(vocab))\n",
    "    for word in item_content[i]:\n",
    "        d_i[vocab.index(word)] = tf_top_i[word]/len(item_content[i]) * idf[word]\n",
    "    return(d_i)\n",
    "\n",
    "def Content_similarity(i,j):\n",
    "    d_i,d_j = tf_idf(i),tf_idf(j)\n",
    "    s_ij = 1 - cosine(d_i,d_j)\n",
    "    return(s_ij)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "def predict_score(u,i,sim_func=Pearson_correlation):\n",
    "    top = 0\n",
    "    bot = 0\n",
    "    for j in user_item_rating[u].keys():\n",
    "        b_uj = b_u[u]+b_i[j]+mu\n",
    "        s_ij = sim_func(i,j)\n",
    "        top += s_ij*(user_item_rating[u][j]-b_uj)\n",
    "        bot += s_ij\n",
    "    score = b_u[u] + b_i[i] + mu + top/bot\n",
    "    return round(score,1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "print('Pearson correlation')\n",
    "print('user\\t', 'item\\t', 'score\\t')\n",
    "for user,item in test:\n",
    "    print(user,'\\t',item,'\\t',predict_score(int(user),int(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Content similarity')\n",
    "print('user\\t', 'item\\t', 'score\\t')\n",
    "for user,item in test:\n",
    "    print(user,'\\t',item,'\\t',predict_score(int(user),int(item),sim_func=Content_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
